{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Kratos About Kratos is a machine learning project developed by a team of Computer Science students at Portland State University for their Capstone . The Kratos project uses Tensorflow and PyTorch frameworks to render predictions of clothing descriptions from an image. Models were trained using images from the Deep Fashion Database developed by researchers from the Chinese University of Hong Kong. Project Participants Adam Kowalski Jordan Le Ray Emory Scot Lambert Yikun Han Yu Li Zack Salah","title":"Introduction"},{"location":"#welcome-to-kratos","text":"","title":"Welcome to Kratos"},{"location":"#about","text":"Kratos is a machine learning project developed by a team of Computer Science students at Portland State University for their Capstone . The Kratos project uses Tensorflow and PyTorch frameworks to render predictions of clothing descriptions from an image. Models were trained using images from the Deep Fashion Database developed by researchers from the Chinese University of Hong Kong.","title":"About"},{"location":"#project-participants","text":"Adam Kowalski Jordan Le Ray Emory Scot Lambert Yikun Han Yu Li Zack Salah","title":"Project Participants"},{"location":"contact/","text":"Contact Information Scot Lambert sll6850@gmail.com - (503)-704-7402 Yu Li liyu_0705@outlook.com - (503)-847-8273 Jordan Le jor25@pdx.edu Yikun Han han.yikun5250@pdx.edu Ray Emory rtsemory@gmail.com","title":"Contact"},{"location":"contact/#contact-information","text":"Scot Lambert sll6850@gmail.com - (503)-704-7402 Yu Li liyu_0705@outlook.com - (503)-847-8273 Jordan Le jor25@pdx.edu Yikun Han han.yikun5250@pdx.edu Ray Emory rtsemory@gmail.com","title":"Contact Information"},{"location":"dependencies/","text":"Kratos Dependencies As Kratos was developed by a team working in parallel, yet independently, certain models have separate requirements. To get around this, the use of the Conda package and environment manager was implemented. Each model, and the framework as a whole has a yaml file associated with it for a Conda environment setup. Installing Conda The Conda environment manager can be installed with either the full Anaconda or Miniconda packages. Instructions for the installation of Conda can be found here . Setting up a Conda environment Included in the repository files is an enviroment.yml file, contained in /app/backend/ . To setup a Conda environment using this file, use the following command line sequence: conda env create -f environment.yml Your Conda environment is now setup. Setup will not need to be run again. To activate your new Conda environment, use the following command: conda activate BackendEnv Your Conda environment is now installed and activated. This environment contains the dependencies for all models in the repository and the UI. To deactivate the environment, use the following command: conda deactivate","title":"Dependencies"},{"location":"dependencies/#kratos-dependencies","text":"As Kratos was developed by a team working in parallel, yet independently, certain models have separate requirements. To get around this, the use of the Conda package and environment manager was implemented. Each model, and the framework as a whole has a yaml file associated with it for a Conda environment setup.","title":"Kratos Dependencies"},{"location":"dependencies/#installing-conda","text":"The Conda environment manager can be installed with either the full Anaconda or Miniconda packages. Instructions for the installation of Conda can be found here .","title":"Installing Conda"},{"location":"dependencies/#setting-up-a-conda-environment","text":"Included in the repository files is an enviroment.yml file, contained in /app/backend/ . To setup a Conda environment using this file, use the following command line sequence: conda env create -f environment.yml Your Conda environment is now setup. Setup will not need to be run again. To activate your new Conda environment, use the following command: conda activate BackendEnv Your Conda environment is now installed and activated. This environment contains the dependencies for all models in the repository and the UI. To deactivate the environment, use the following command: conda deactivate","title":"Setting up a Conda environment"},{"location":"labels/","text":"Expanding Labels To expand the labels for which a given model makes predictions will require two steps. 1. Gathering relevant labeled images. 2. Retraining the existing models . Colors To add additional colors to the model, images will need to be added to the In-shop Clothes Retrieval Benchmark/Img directory. Images should be added within a uniquely named sub-directory. Image and label data will need to be added to the list_color_cloth.txt file within the In-shop Clothes Retrieval Benchmark/Anno directory. For every image added in the Img/ directory, add a line to the list_color_cloth.txt file. Include the path to the image and a text descriptor of the color of the clothes in that image. Next modify the number in the first line of the text file. For every added image, increment the number by 1. Next modify the list_eval_partition.txt file in the In-shop Clothes Retrieval Benchmark/Eval directory. For every image added in the Img/ directory, add a line to the list_eval_partition.txt file. Include the path to the image and either \"train\" or \"test\" to determine if the image will be used in training data or testing data. The current ratio for Train:Test is approximately 3:1. The color model is now ready for re-training Categories To add additional categories to the model, images will need to be added to the Category and Attribute Prediction Benchmark/Img/ directory. Images should be added within a uniquely named sub-directory. Label data will need to be added to two of the text files within Category and Attribute Prediction Benchmark/Anno/ list_category_cloth.txt list_category_img.txt Add the new label name to the end of list_category_cloth.txt and a descriptor 1-3 on the same line. 1. Upper Body category 2. Lower Body category 3. Full Body category Next modify the number in the first line of the text file. For every added category, increment the number by 1. For every image added in the Img/ directory add a line to the list_category_img.txt file. Include the path to the image and a number descriptor of the image. The number descriptor corresponds with the index of the new label (Starts at 1). Ex. If you add one new label to the existing data, new images will have label 51, as there are currently 50 labels. Next modify the number on the first line of the text file. For every added image, increment the number by 1. Next modify the list_eval_partition.txt file in the Category and Attribute Prediction Benchmark/Eval/ directory. For every image added in the Img/ directory, add a line to the list_eval_partition.txt file. Include the path to the image and either \"train\" or \"test\" to determine if the image will be used in training data or testing data. The current ratio for Train:Test is approximately 4:1. The category models are now ready for re-training . Attributes To add additional attributes to the model, images will need to be added to the Category and Attribute Prediction Benchmark/Img/ directory. Images should be added within a uniquely named sub-directory. Label data will need to be added to two of the text files within Category and Attribute Prediction Benchmark/Anno/ list_attr_cloth.txt list_attr_img.txt Add the new label name to the end of list_attr_cloth.txt and a descriptor 1-5 on the same line. 1. Texture attributes 2. Fabric attributes 3. Shape attributes 4. Part attributes 5. Style attributes Next modify the number in the first line of the text file. For every added attribute, increment the number by 1. For every image added in the Img/ directory add a line to the list_attr_img.txt file. The line should include the path to the image and N elements of -1 or 1 separated by a space. Where N is the total number of attribute labels. For every element of N on that line, -1 denotes that the image does not have that particular attribute and 1 denotes that that attribute is present. Additionally, every existing image path label will need to be updated to reflect its possession of the new attribute. Lastly modify the number on the first line of the text file. For every added image, increment the number by 1. Next modify the list_eval_partition.txt file in the Category and Attribute Prediction Benchmark/Eval/ directory. For every image added in the Img/ directory, add a line to the list_eval_partition.txt file. Include the path to the image and either \"train\" or \"test\" to determine if the image will be used in training data or testing data. The current ratio for Train:Test is approximately 4:1. The Attribute models are now ready for re-training .","title":"Label Expansion"},{"location":"labels/#expanding-labels","text":"To expand the labels for which a given model makes predictions will require two steps. 1. Gathering relevant labeled images. 2. Retraining the existing models .","title":"Expanding Labels"},{"location":"labels/#colors","text":"To add additional colors to the model, images will need to be added to the In-shop Clothes Retrieval Benchmark/Img directory. Images should be added within a uniquely named sub-directory. Image and label data will need to be added to the list_color_cloth.txt file within the In-shop Clothes Retrieval Benchmark/Anno directory. For every image added in the Img/ directory, add a line to the list_color_cloth.txt file. Include the path to the image and a text descriptor of the color of the clothes in that image. Next modify the number in the first line of the text file. For every added image, increment the number by 1. Next modify the list_eval_partition.txt file in the In-shop Clothes Retrieval Benchmark/Eval directory. For every image added in the Img/ directory, add a line to the list_eval_partition.txt file. Include the path to the image and either \"train\" or \"test\" to determine if the image will be used in training data or testing data. The current ratio for Train:Test is approximately 3:1. The color model is now ready for re-training","title":"Colors"},{"location":"labels/#categories","text":"To add additional categories to the model, images will need to be added to the Category and Attribute Prediction Benchmark/Img/ directory. Images should be added within a uniquely named sub-directory. Label data will need to be added to two of the text files within Category and Attribute Prediction Benchmark/Anno/ list_category_cloth.txt list_category_img.txt Add the new label name to the end of list_category_cloth.txt and a descriptor 1-3 on the same line. 1. Upper Body category 2. Lower Body category 3. Full Body category Next modify the number in the first line of the text file. For every added category, increment the number by 1. For every image added in the Img/ directory add a line to the list_category_img.txt file. Include the path to the image and a number descriptor of the image. The number descriptor corresponds with the index of the new label (Starts at 1). Ex. If you add one new label to the existing data, new images will have label 51, as there are currently 50 labels. Next modify the number on the first line of the text file. For every added image, increment the number by 1. Next modify the list_eval_partition.txt file in the Category and Attribute Prediction Benchmark/Eval/ directory. For every image added in the Img/ directory, add a line to the list_eval_partition.txt file. Include the path to the image and either \"train\" or \"test\" to determine if the image will be used in training data or testing data. The current ratio for Train:Test is approximately 4:1. The category models are now ready for re-training .","title":"Categories"},{"location":"labels/#attributes","text":"To add additional attributes to the model, images will need to be added to the Category and Attribute Prediction Benchmark/Img/ directory. Images should be added within a uniquely named sub-directory. Label data will need to be added to two of the text files within Category and Attribute Prediction Benchmark/Anno/ list_attr_cloth.txt list_attr_img.txt Add the new label name to the end of list_attr_cloth.txt and a descriptor 1-5 on the same line. 1. Texture attributes 2. Fabric attributes 3. Shape attributes 4. Part attributes 5. Style attributes Next modify the number in the first line of the text file. For every added attribute, increment the number by 1. For every image added in the Img/ directory add a line to the list_attr_img.txt file. The line should include the path to the image and N elements of -1 or 1 separated by a space. Where N is the total number of attribute labels. For every element of N on that line, -1 denotes that the image does not have that particular attribute and 1 denotes that that attribute is present. Additionally, every existing image path label will need to be updated to reflect its possession of the new attribute. Lastly modify the number on the first line of the text file. For every added image, increment the number by 1. Next modify the list_eval_partition.txt file in the Category and Attribute Prediction Benchmark/Eval/ directory. For every image added in the Img/ directory, add a line to the list_eval_partition.txt file. Include the path to the image and either \"train\" or \"test\" to determine if the image will be used in training data or testing data. The current ratio for Train:Test is approximately 4:1. The Attribute models are now ready for re-training .","title":"Attributes"},{"location":"license/","text":"License Kratos is developed and distributed under the MIT open source license. MCopyright 2019 [A. Kowalski, J. Le, R. Emory, S. Lambert, Y. Han, Y. Li, Z. Salah] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"Kratos is developed and distributed under the MIT open source license. MCopyright 2019 [A. Kowalski, J. Le, R. Emory, S. Lambert, Y. Han, Y. Li, Z. Salah] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"models/","text":"Kratos Machine Learning Models Included in the Kratos packages are 6 machine learning models, 5 developed with the Tensorflow framework and 1 with the PyTorch framework. These models will make predictions on an image of an article of clothing, predictions include the color of the item, the category of clothing it falls under, and some attributes of the item. Predictions Color The color prediction model was developed in the PyTorch framework to predict the color of the clothes in images. The initial training images and labels are from the DeepFashion dataset In-Shop Clothes Retrieval Benchmark partition. At current stage, the model is capable of predicting one result like Oatmeal-indigo, Wine-cream and Orange-navy for an image that contains a person wearing clothing of multiple colors. Data from the Deep Fashion dataset has been pre-processed into a set of five text files. allcolor.txt - The list of color labels traindataset.txt - The list of images from the Deep fashion dataset used for the training data testdataset.txt - The list of images from the Deep Fashion dataset used for the testing data trainlabelset.txt - The list of labels accompanying the traindataset.txt data testlabelset.txt - The list of labels accompanying the testdataset.txt data These files are generated by the datapipline_update.py file from the list_color_cloth.txt and list_eval_partition.txt files contained in the In-Shop Clothes Retrieval Benchmark partition Anno and Eval directories, respectively. Categorical The categorical prediction models were developed using Tensorflow to predict categorical information about clothing in images. The initial training images and labels are from the DeepFashion dataset Category and Attribute Prediction Benchmark partition. Attribute The attribute prediction models were developed with the Tensorflow framework to predict a set of attributes about clothing in an image. The initial training images and labels are from the DeepFashion dataset Category and Attribute Prediction Benchmark partition. Attributes refers to descriptions of five separate types; the texture, fabric, shape, part, or style of an article of clothing. The attribute predictions may return multiple instances of an attribute type with each prediction. We found in our development that the large selection of attribute options was detrimental to the machine learning. Attributes model #1 separates each of the 5 categories into a separate machine learning task. This was able to vastly improve the capabilities of accurate predictions when compared with Attributes model #2 . Run a predictor Please see the UI page for operation of the Kratos models through the UI interface. To run a prediction of a single model over an image or a set of images, the following files within that model architecture will need to be modified and executed. Predict Color To make a prediction using the color prediction model, execute the img_test.py file. Modifications to the script will need to be made for the path of the desired image path and the path to the weights test.pth . This script needs the allcolor.txt file generated by the datapipline_update.py script. Predict Category #1 To make a prediction using category prediction model #1, a new .py file will have to be created. import tensorflow as tf from tensorflow import keras import data_precess as dp import reload_model as rm import category_model as cm model = cm.create_model() model.load_weights('model_weights.h5') #<file> can be either a path to an image, or a text file containing multiple image paths, one per line. predictions = rm.predict(model, <file>) print(predictions) The category model #1 will return the top 5 predictions for each image. Predict Category #2 To make a prediction using category prediction model #2, a new .py file will have to be created. from inference import predict # <file> is the path to an image predictions = predict(<file>) print(predictions) The category model #2 will return the top 5 predictions for the image. Predict Category #3 To make a prediction using category prediction model #3, execute the predict_vgg19.py file. Modifications to the script will need to be made for the filenames variable. Create a list with the paths of each image for which you wish to make a prediction. Predict Attribute #1 To make a prediction using attribute prediction model #1, execute the Agent.py file. Modifications to the script will need to be made for the image_path and dir_models variables; for the paths to the image on which to predict and the path to the model respectively. Predict Attribute #2 To make a prediction using attributes prediction model #2, execute the runModel.py file. Pass the -i <image_path> argument to run the model over a specific image. Pass the -m <model_path> argument to load specific weights. The FLAGS.test_list variable can point to a text file containing image paths (one per line). If the -i argument is not used, predictions will be made over images found within this text file. Retraining To execute a retraining over the same or a new dataset, the following files will need to be modified and executed. Retrain Color The python file capstone_test.py contains the code for training the color predictor model. To retrain the color model, the four train and test files generated by the datapipeline_update.py are required. To vary the number of epochs the model will train over, edit the range over which intEpoch is run. To save the model after each epoch, uncomment the final line. To change the name of the save file, change ./test.pth Retrain Category #1 The python file train_test.py contains the code for training the category predictor model #1. To retrain this model, execute the train_test.py python file, epochs can be modified within the Info.epochs variable The path to the main dataset file will need to be modified, the PROPERTY.path variable in the data_processor.py file. If training with expanded labels , the PROPERTY.CATEGORIES list in data_processor.py will need to be manually updated with the new category names. Retrain Category #2 The python file model.py contains the code necessary to train the model. You can modify the architecture of the network, or tweak other hyperparameters there. Additionally by either changing the file paths, or appending to the list_eval_partition.txt file you can choose which images you want to train on as well as which should be in your validation, and test sets. The path to the main dataset directory ill need to be modified, the FLAGS.data_dir variable in model.py . If training with expanded labels , the FLAGS.classes list in model.py will need to be manually updated with the new number of categories. Retrain Category #3 The python file load_and_train_vgg19.py contains the code for training the category predictor model #3. To retrain this model, execute the load_and_train_vgg19.py . This code is designed to run one epoch. Further epochs are to be run from the continue_training_vgg19.py file. This file will use the weights created by load_and_train_vgg19.py for continued training. The path to the main dataset directory will need to be modified, the MODDED.data_dir variable in the model_architecture_vgg19.py file. If training with expanded labels , the MODDED.classes and MODDED.CATEGORIES list in model_architecture_vgg19.py will need to be manually updated with the new number of categories and new category names. Retrain Attribute #1 The python file KratosAttributesTrainingModel.py contains the code for training the attributes predictor model #1. To retrain this model, new pickle format data files will need to be created. To do this, execute the DeepFashionDataPreprocessing.py script. This expects the list_attr_cloth.txt & list_eval_partition files from the Category and Attribute Prediction Benchmark directory to be in the same directory as DeepFashionDataPreprocessing.py . This wil create two files, DataSet.pickle & Attributes.pickle . Once the .pickle files have been created, KratosAttributesTraininModel.py can be run. This model has split the 5 sub-categories of attributes into separate learning tasks and will run for each of them sequentially. Retrain Attribute #2 The python file attributes.py contains the code for training the attributes predictor #2. To retrain this model, execute the attributes.py file. The path to the main dataset directory will need to be modified, the FLAGS.data_dir variable in the model_setup.py file. If training with expanded labels , the FLAGS.classes variable in model_setup.py will need to be manually updated with the new number of attributes.","title":"Models"},{"location":"models/#kratos-machine-learning-models","text":"Included in the Kratos packages are 6 machine learning models, 5 developed with the Tensorflow framework and 1 with the PyTorch framework. These models will make predictions on an image of an article of clothing, predictions include the color of the item, the category of clothing it falls under, and some attributes of the item.","title":"Kratos Machine Learning Models"},{"location":"models/#predictions","text":"","title":"Predictions"},{"location":"models/#color","text":"The color prediction model was developed in the PyTorch framework to predict the color of the clothes in images. The initial training images and labels are from the DeepFashion dataset In-Shop Clothes Retrieval Benchmark partition. At current stage, the model is capable of predicting one result like Oatmeal-indigo, Wine-cream and Orange-navy for an image that contains a person wearing clothing of multiple colors. Data from the Deep Fashion dataset has been pre-processed into a set of five text files. allcolor.txt - The list of color labels traindataset.txt - The list of images from the Deep fashion dataset used for the training data testdataset.txt - The list of images from the Deep Fashion dataset used for the testing data trainlabelset.txt - The list of labels accompanying the traindataset.txt data testlabelset.txt - The list of labels accompanying the testdataset.txt data These files are generated by the datapipline_update.py file from the list_color_cloth.txt and list_eval_partition.txt files contained in the In-Shop Clothes Retrieval Benchmark partition Anno and Eval directories, respectively.","title":"Color"},{"location":"models/#categorical","text":"The categorical prediction models were developed using Tensorflow to predict categorical information about clothing in images. The initial training images and labels are from the DeepFashion dataset Category and Attribute Prediction Benchmark partition.","title":"Categorical"},{"location":"models/#attribute","text":"The attribute prediction models were developed with the Tensorflow framework to predict a set of attributes about clothing in an image. The initial training images and labels are from the DeepFashion dataset Category and Attribute Prediction Benchmark partition. Attributes refers to descriptions of five separate types; the texture, fabric, shape, part, or style of an article of clothing. The attribute predictions may return multiple instances of an attribute type with each prediction. We found in our development that the large selection of attribute options was detrimental to the machine learning. Attributes model #1 separates each of the 5 categories into a separate machine learning task. This was able to vastly improve the capabilities of accurate predictions when compared with Attributes model #2 .","title":"Attribute"},{"location":"models/#run-a-predictor","text":"Please see the UI page for operation of the Kratos models through the UI interface. To run a prediction of a single model over an image or a set of images, the following files within that model architecture will need to be modified and executed.","title":"Run a predictor"},{"location":"models/#predict-color","text":"To make a prediction using the color prediction model, execute the img_test.py file. Modifications to the script will need to be made for the path of the desired image path and the path to the weights test.pth . This script needs the allcolor.txt file generated by the datapipline_update.py script.","title":"Predict Color"},{"location":"models/#predict-category-1","text":"To make a prediction using category prediction model #1, a new .py file will have to be created. import tensorflow as tf from tensorflow import keras import data_precess as dp import reload_model as rm import category_model as cm model = cm.create_model() model.load_weights('model_weights.h5') #<file> can be either a path to an image, or a text file containing multiple image paths, one per line. predictions = rm.predict(model, <file>) print(predictions) The category model #1 will return the top 5 predictions for each image.","title":"Predict Category #1"},{"location":"models/#predict-category-2","text":"To make a prediction using category prediction model #2, a new .py file will have to be created. from inference import predict # <file> is the path to an image predictions = predict(<file>) print(predictions) The category model #2 will return the top 5 predictions for the image.","title":"Predict Category #2"},{"location":"models/#predict-category-3","text":"To make a prediction using category prediction model #3, execute the predict_vgg19.py file. Modifications to the script will need to be made for the filenames variable. Create a list with the paths of each image for which you wish to make a prediction.","title":"Predict Category #3"},{"location":"models/#predict-attribute-1","text":"To make a prediction using attribute prediction model #1, execute the Agent.py file. Modifications to the script will need to be made for the image_path and dir_models variables; for the paths to the image on which to predict and the path to the model respectively.","title":"Predict Attribute #1"},{"location":"models/#predict-attribute-2","text":"To make a prediction using attributes prediction model #2, execute the runModel.py file. Pass the -i <image_path> argument to run the model over a specific image. Pass the -m <model_path> argument to load specific weights. The FLAGS.test_list variable can point to a text file containing image paths (one per line). If the -i argument is not used, predictions will be made over images found within this text file.","title":"Predict Attribute #2"},{"location":"models/#retraining","text":"To execute a retraining over the same or a new dataset, the following files will need to be modified and executed.","title":"Retraining"},{"location":"models/#retrain-color","text":"The python file capstone_test.py contains the code for training the color predictor model. To retrain the color model, the four train and test files generated by the datapipeline_update.py are required. To vary the number of epochs the model will train over, edit the range over which intEpoch is run. To save the model after each epoch, uncomment the final line. To change the name of the save file, change ./test.pth","title":"Retrain Color"},{"location":"models/#retrain-category-1","text":"The python file train_test.py contains the code for training the category predictor model #1. To retrain this model, execute the train_test.py python file, epochs can be modified within the Info.epochs variable The path to the main dataset file will need to be modified, the PROPERTY.path variable in the data_processor.py file. If training with expanded labels , the PROPERTY.CATEGORIES list in data_processor.py will need to be manually updated with the new category names.","title":"Retrain Category #1"},{"location":"models/#retrain-category-2","text":"The python file model.py contains the code necessary to train the model. You can modify the architecture of the network, or tweak other hyperparameters there. Additionally by either changing the file paths, or appending to the list_eval_partition.txt file you can choose which images you want to train on as well as which should be in your validation, and test sets. The path to the main dataset directory ill need to be modified, the FLAGS.data_dir variable in model.py . If training with expanded labels , the FLAGS.classes list in model.py will need to be manually updated with the new number of categories.","title":"Retrain Category #2"},{"location":"models/#retrain-category-3","text":"The python file load_and_train_vgg19.py contains the code for training the category predictor model #3. To retrain this model, execute the load_and_train_vgg19.py . This code is designed to run one epoch. Further epochs are to be run from the continue_training_vgg19.py file. This file will use the weights created by load_and_train_vgg19.py for continued training. The path to the main dataset directory will need to be modified, the MODDED.data_dir variable in the model_architecture_vgg19.py file. If training with expanded labels , the MODDED.classes and MODDED.CATEGORIES list in model_architecture_vgg19.py will need to be manually updated with the new number of categories and new category names.","title":"Retrain Category #3"},{"location":"models/#retrain-attribute-1","text":"The python file KratosAttributesTrainingModel.py contains the code for training the attributes predictor model #1. To retrain this model, new pickle format data files will need to be created. To do this, execute the DeepFashionDataPreprocessing.py script. This expects the list_attr_cloth.txt & list_eval_partition files from the Category and Attribute Prediction Benchmark directory to be in the same directory as DeepFashionDataPreprocessing.py . This wil create two files, DataSet.pickle & Attributes.pickle . Once the .pickle files have been created, KratosAttributesTraininModel.py can be run. This model has split the 5 sub-categories of attributes into separate learning tasks and will run for each of them sequentially.","title":"Retrain Attribute #1"},{"location":"models/#retrain-attribute-2","text":"The python file attributes.py contains the code for training the attributes predictor #2. To retrain this model, execute the attributes.py file. The path to the main dataset directory will need to be modified, the FLAGS.data_dir variable in the model_setup.py file. If training with expanded labels , the FLAGS.classes variable in model_setup.py will need to be manually updated with the new number of attributes.","title":"Retrain Attribute #2"},{"location":"real_use/","text":"Paths Forward Deployment Every model is packaged behind a rest api with a similar interface. This is done intentionally so that they can be incorporated into a larger system with ease as micro services. Any product which could be improved through the ability to extract fashion information out of images can benefit from this tool. By allowing them to be deployed as independent servers, they can be used from a cross language development environment, as well as from any user interface, either mobile or web. Improvement The models themselves can be improved in several ways if desired. Several models were created to predict the categories as well as attributes of an image. By utilizing ensemble learning, you can combine the results of several models to get higher accuracy classification at the cost of more compute. The model architectures themselves can be altered, either through utilizing larger pre-trained models for use with transfer learning, or tweaking the number of convolutional layers. New data can be introduced as well, which can greatly improve the generalization of the models which may be beneficial if the test data distribution doesn't match that of the images we trained on. Transfer Learning In deep learning, weight initialization is a big issue which can dramatically impact how long learning to solve a new problem can take. As these models are now trained on fashion images already, they can simplify the task of learning novel fashion related tasks, by simply reusing the models that are built here, and replacing the final dense layers. This can give a large business advantage versus competitors who are starting from randomly initialized models and don't have hundreds of thousands of images already improving their models. Web Scraping Scraping various social media outlets in an automated way can be a big way to understand the fashion sense of the current trend setters. Building a style profile of those who are influential to the consumers can drive you towards new designs and products that will sell well by riding trends while they are still relevant. Reducing the time to market can make a drastic impact on the ability to reach your target audience and generate new buyers.","title":"Paths Forward"},{"location":"real_use/#paths-forward","text":"","title":"Paths Forward"},{"location":"real_use/#deployment","text":"Every model is packaged behind a rest api with a similar interface. This is done intentionally so that they can be incorporated into a larger system with ease as micro services. Any product which could be improved through the ability to extract fashion information out of images can benefit from this tool. By allowing them to be deployed as independent servers, they can be used from a cross language development environment, as well as from any user interface, either mobile or web.","title":"Deployment"},{"location":"real_use/#improvement","text":"The models themselves can be improved in several ways if desired. Several models were created to predict the categories as well as attributes of an image. By utilizing ensemble learning, you can combine the results of several models to get higher accuracy classification at the cost of more compute. The model architectures themselves can be altered, either through utilizing larger pre-trained models for use with transfer learning, or tweaking the number of convolutional layers. New data can be introduced as well, which can greatly improve the generalization of the models which may be beneficial if the test data distribution doesn't match that of the images we trained on.","title":"Improvement"},{"location":"real_use/#transfer-learning","text":"In deep learning, weight initialization is a big issue which can dramatically impact how long learning to solve a new problem can take. As these models are now trained on fashion images already, they can simplify the task of learning novel fashion related tasks, by simply reusing the models that are built here, and replacing the final dense layers. This can give a large business advantage versus competitors who are starting from randomly initialized models and don't have hundreds of thousands of images already improving their models.","title":"Transfer Learning"},{"location":"real_use/#web-scraping","text":"Scraping various social media outlets in an automated way can be a big way to understand the fashion sense of the current trend setters. Building a style profile of those who are influential to the consumers can drive you towards new designs and products that will sell well by riding trends while they are still relevant. Reducing the time to market can make a drastic impact on the ability to reach your target audience and generate new buyers.","title":"Web Scraping"},{"location":"ui/","text":"Kratos -- UI -- By Zack Salah backend\\predictors by A. Kowalski, J. Le, R. Emory, Y. Han, Y. Li, Z. Salah and modified by Zack Salah backend\\models by A. Kowalski, J. Le, R. Emory, Y. Han, Y. Li, Z. Salah Technologies Language : Python, JavaScript JavaScript framework : React Native React native toolchain : Expo Python distribution : Anaconda Parallel computing architecture : CUDA Libraries : Tensorflow, Flask, OpenCV, Pytorch, Matplotlib, Pandas Using The UI Dependencies: Frontend nodeJS and Expo Download and install node Download expo-cli npm install expo-cli --global Emulator specific Download and install Android Studio Backend Anaconda Install Anaconda DeepFashion txt files From the Deep Fashion Dataset Category and Attribute Prediction Benchmark partition download the following folders: anno eval Add them to the backend\\deep-fashion folder Environment installation In the frontend folder you will notice environment.yml file. Create an environment with this .yml file. Note: If you want to make the environment in a specific path, simply add -p path\\environment_name at the end of the command conda env create -f environment.yml Lunching backend source activate installed_environment python run.py Lunching frontend Go to App\\frontend\\app\\component\\ImageUploader\\ImageUploader.js and modify the apiUrl variable at line 71 to the backends' url Go back frontend directory and run the frontend expo start Connecting your device to the UI Connecting your device to the machine. Activate USB tethering on you mobile device Locate the IP address Go to App\\frontend\\app\\component\\ImageUploader\\ImageUploader.js and modify the apiUrl variable at line 71 to mobile device IP Connecting an emulator to the UI - pixel 2 preferred Lunch the emulator. Make sure you have your frontend running. Go to the cmd where you ran your frontend and press d. Once a tab open on the browser, click on run on android device/emulator Expected format The backend expects a formdata with a image url embedded in it. The Key value is the photo. UI output The output will be a the image taken via camera or from picture library. Then the predictions in text format. The output should be as follows: | Image | | Predictions | | | | Color: xxxx | | | | Top Five categories: | | 1st Model: x,x,x,x,x | | 2nd Model: x,x,x,x,x | | 3rd Model: x,x,x,x,x | | | | Attributes | | Texture: x,x.... | | Fabric: x,x.... | | Shape: x,x.... | | Part: x,x.... | | Style: x,x.... | interpreting data The Models for this project is not 100% accurate. The color and category models are about 50% while the attributes are about 10% accurate. The Category models gives a list of five highest categories. The left most category being the highest and right most is the lowest. Top 5 prediction accuracy is about 90% The Attributes predictions gives a list of the highest attributes that surpassed a certain threshold.","title":"UI Use"},{"location":"ui/#kratos-ui-","text":"By Zack Salah backend\\predictors by A. Kowalski, J. Le, R. Emory, Y. Han, Y. Li, Z. Salah and modified by Zack Salah backend\\models by A. Kowalski, J. Le, R. Emory, Y. Han, Y. Li, Z. Salah Technologies Language : Python, JavaScript JavaScript framework : React Native React native toolchain : Expo Python distribution : Anaconda Parallel computing architecture : CUDA Libraries : Tensorflow, Flask, OpenCV, Pytorch, Matplotlib, Pandas","title":"Kratos -- UI --"},{"location":"ui/#using-the-ui","text":"","title":"Using The UI"},{"location":"ui/#dependencies","text":"","title":"Dependencies:"},{"location":"ui/#frontend","text":"","title":"Frontend"},{"location":"ui/#nodejs-and-expo","text":"Download and install node Download expo-cli npm install expo-cli --global","title":"nodeJS and Expo"},{"location":"ui/#emulator-specific","text":"Download and install Android Studio","title":"Emulator specific"},{"location":"ui/#backend","text":"","title":"Backend"},{"location":"ui/#anaconda","text":"Install Anaconda","title":"Anaconda"},{"location":"ui/#deepfashion-txt-files","text":"From the Deep Fashion Dataset Category and Attribute Prediction Benchmark partition download the following folders: anno eval Add them to the backend\\deep-fashion folder","title":"DeepFashion txt files"},{"location":"ui/#environment-installation","text":"In the frontend folder you will notice environment.yml file. Create an environment with this .yml file. Note: If you want to make the environment in a specific path, simply add -p path\\environment_name at the end of the command conda env create -f environment.yml","title":"Environment installation"},{"location":"ui/#lunching-backend","text":"source activate installed_environment python run.py","title":"Lunching backend"},{"location":"ui/#lunching-frontend","text":"Go to App\\frontend\\app\\component\\ImageUploader\\ImageUploader.js and modify the apiUrl variable at line 71 to the backends' url Go back frontend directory and run the frontend expo start","title":"Lunching frontend"},{"location":"ui/#connecting-your-device-to-the-ui","text":"Connecting your device to the machine. Activate USB tethering on you mobile device Locate the IP address Go to App\\frontend\\app\\component\\ImageUploader\\ImageUploader.js and modify the apiUrl variable at line 71 to mobile device IP","title":"Connecting your device to the UI"},{"location":"ui/#connecting-an-emulator-to-the-ui-pixel-2-preferred","text":"Lunch the emulator. Make sure you have your frontend running. Go to the cmd where you ran your frontend and press d. Once a tab open on the browser, click on run on android device/emulator","title":"Connecting an emulator to the UI - pixel 2 preferred"},{"location":"ui/#expected-format","text":"The backend expects a formdata with a image url embedded in it. The Key value is the photo.","title":"Expected format"},{"location":"ui/#ui-output","text":"The output will be a the image taken via camera or from picture library. Then the predictions in text format. The output should be as follows: | Image | | Predictions | | | | Color: xxxx | | | | Top Five categories: | | 1st Model: x,x,x,x,x | | 2nd Model: x,x,x,x,x | | 3rd Model: x,x,x,x,x | | | | Attributes | | Texture: x,x.... | | Fabric: x,x.... | | Shape: x,x.... | | Part: x,x.... | | Style: x,x.... |","title":"UI output"},{"location":"ui/#interpreting-data","text":"The Models for this project is not 100% accurate. The color and category models are about 50% while the attributes are about 10% accurate. The Category models gives a list of five highest categories. The left most category being the highest and right most is the lowest. Top 5 prediction accuracy is about 90% The Attributes predictions gives a list of the highest attributes that surpassed a certain threshold.","title":"interpreting data"}]}